{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Importing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "from scipy import stats\n",
    "from google.colab import drive\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Visualization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: defining variable and preparing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/content/synthetic_stock_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: first visualization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verificar si el archivo existe en la ruta ---\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"Archivo encontrado en: {csv_path}\")\n",
    "    # --- Cargar el conjunto de datos ---\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(\"¡DataFrame cargado exitosamente!\")\n",
    "\n",
    "        # ==================================================\n",
    "        # 1.    Carga inicial del proyecto.\n",
    "        # ==================================================\n",
    "        print(\"\\n----- 1. Carga e Inspección Inicial -----\")\n",
    "\n",
    "        # Ver las primeras y últimas filas\n",
    "        print(\"\\nPrimeras 5 filas:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nÚltimas 5 filas:\")\n",
    "        print(df.tail())\n",
    "\n",
    "        # Obtener dimensiones\n",
    "        print(f\"\\nDimensiones del DataFrame (filas, columnas): {df.shape}\")\n",
    "\n",
    "        # Revisar tipos de datos y valores no nulos\n",
    "        print(\"\\nInformación del DataFrame (Tipos de datos y Nulos):\")\n",
    "        df.info()\n",
    "\n",
    "        # Obtener estadísticas descriptivas básicas para variables numéricas\n",
    "        print(\"\\nEstadísticas Descriptivas (Variables Numéricas):\")\n",
    "        print(df.describe().T) # Usar .T para transponer y facilitar la lectura\n",
    "\n",
    "        # Contar valores únicos por columna\n",
    "        print(\"\\nConteo de Valores Únicos por Columna:\")\n",
    "        print(df.nunique())\n",
    "\n",
    "        # ... (Aquí pegarías el resto del código del EDA: Limpieza, Univariado, Bivariado, etc.) ...\n",
    "        # El resto del código que te proporcioné anteriormente funcionará\n",
    "        # a partir de aquí, ya que la variable 'df' ahora contiene tus datos.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo CSV: {e}\")\n",
    "        print(\"Asegúrate de que el archivo se subió correctamente y no está corrupto.\")\n",
    "\n",
    "else:\n",
    "    print(f\"¡ERROR! No se encontró el archivo en la ruta: {csv_path}\")\n",
    "    print(\"Por favor, asegúrate de haber subido el archivo 'synthetic_stock_data.csv' a tu sesión actual de Colab.\")\n",
    "    print(\"Puedes hacerlo usando el botón 'Subir archivo' en el panel de archivos de la izquierda.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Manejo de Duplicados ---\n",
    "print(\"\\nNúmero de Filas Duplicadas:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(duplicates)\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"\\nSe encontraron {duplicates} filas duplicadas. Eliminándolas.\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Nuevas dimensiones del DataFrame: {df.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo se encontraron filas duplicadas.\")\n",
    "\n",
    "# --- Corrección de Tipos de Datos ---\n",
    "print(\"\\nVerificando y corrigiendo tipos de datos...\")\n",
    "# Convertir 'Date' a datetime si no lo está ya\n",
    "if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "    try:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        print(\"'Date' convertida a tipo datetime.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al convertir 'Date': {e}. Se mantendrá como objeto.\")\n",
    "else:\n",
    "    print(\"'Date' ya es de tipo datetime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2. Análisis Univariado\n",
    "# ==================================================\n",
    "print(\"\\n----- 2. Análisis Univariado -----\")\n",
    "\n",
    "# Identificar columnas numéricas y categóricas (RE-IDENTIFICAR después de limpieza/corrección)\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "date_col = 'Date' # Definir explícitamente\n",
    "# Asegurarse de no incluir columnas no deseadas si su tipo cambió\n",
    "if 'Year' in numeric_cols: numeric_cols.remove('Year') # Si se crearon en una ejecución previa\n",
    "if 'Month' in numeric_cols: numeric_cols.remove('Month')\n",
    "if 'Day' in numeric_cols: numeric_cols.remove('Day')\n",
    "if 'DayOfWeek' in numeric_cols: numeric_cols.remove('DayOfWeek')\n",
    "\n",
    "print(f\"\\nColumnas Numéricas para Análisis Univariado: {numeric_cols}\")\n",
    "print(f\"Columnas Categóricas para Análisis Univariado: {categorical_cols}\")\n",
    "\n",
    "# --- Variables Numéricas ---\n",
    "print(\"\\nAnálisis de Variables Numéricas:\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n--- Análisis de '{col}' ---\")\n",
    "    if df[col].isnull().any():\n",
    "        print(f\"Advertencia: La columna '{col}' contiene valores NaN. Las estadísticas pueden verse afectadas.\")\n",
    "        # Considera df[col].dropna().skew() o .kurt() si quieres estadísticas sin NaNs\n",
    "        skewness = \"N/A (contiene NaN)\"\n",
    "        kurtosis = \"N/A (contiene NaN)\"\n",
    "    else:\n",
    "        skewness = f\"{df[col].skew():.2f}\"\n",
    "        kurtosis = f\"{df[col].kurt():.2f}\"\n",
    "\n",
    "    print(f\"Media: {df[col].mean():.2f}\")\n",
    "    print(f\"Mediana: {df[col].median():.2f}\")\n",
    "    print(f\"Desviación Estándar: {df[col].std():.2f}\")\n",
    "    print(f\"Mínimo: {df[col].min():.2f}\")\n",
    "    print(f\"Máximo: {df[col].max():.2f}\")\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    print(f\"Q1: {Q1:.2f}\")\n",
    "    print(f\"Q3: {Q3:.2f}\")\n",
    "    print(f\"IQR (Rango Intercuartílico): {IQR:.2f}\")\n",
    "    print(f\"Asimetría (Skewness): {skewness}\")\n",
    "    print(f\"Curtosis: {kurtosis}\")\n",
    "\n",
    "    # Visualización: Histograma y Boxplot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Usar dropna() para evitar errores en el plot si hay NaNs\n",
    "    sns.histplot(df[col].dropna(), kde=True)\n",
    "    plt.title(f'Distribución de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df[col].dropna()) # Usar dropna() aquí también\n",
    "    plt.title(f'Boxplot de {col}')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Variables Categóricas ---\n",
    "print(\"\\nAnálisis de Variables Categóricas:\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n--- Análisis de '{col}' ---\")\n",
    "\n",
    "    # Frecuencias y Proporciones\n",
    "    print(\"Conteo de Valores:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    print(value_counts)\n",
    "    print(\"\\nProporción de Valores (%):\")\n",
    "    print(value_counts.div(value_counts.sum()).mul(100).round(2).astype(str) + '%')\n",
    "\n",
    "    # Visualización: Gráfico de Barras (si no hay demasiadas categorías)\n",
    "    num_unique = df[col].nunique()\n",
    "    print(f\"Número de categorías únicas: {num_unique}\")\n",
    "    if num_unique > 0 and num_unique < 30: # Umbral arbitrario y evitar error si columna está vacía\n",
    "        plt.figure(figsize=(10, max(5, num_unique * 0.3))) # Ajustar altura dinámicamente\n",
    "        sns.countplot(y=df[col], order = value_counts.index, palette='viridis')\n",
    "        plt.title(f'Frecuencia de Categorías en {col}')\n",
    "        plt.xlabel('Conteo')\n",
    "        plt.ylabel(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif num_unique >= 30:\n",
    "        print(f\"No se genera gráfico de barras para '{col}' debido a la alta cardinalidad ({num_unique} categorías).\")\n",
    "        print(\"Top 10 categorías:\")\n",
    "        print(value_counts.head(10))\n",
    "    else:\n",
    "         print(f\"La columna '{col}' no tiene valores o categorías únicas para graficar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3. Análisis Bivariado\n",
    "# ==================================================\n",
    "print(\"\\n----- 3. Análisis Bivariado -----\")\n",
    "\n",
    "# --- Numérica vs. Numérica ---\n",
    "print(\"\\nAnálisis Numérica vs. Numérica:\")\n",
    "\n",
    "# Matriz de correlación y Mapa de calor\n",
    "print(\"\\nMatriz de Correlación:\")\n",
    "# Seleccionar solo columnas numéricas que existen en el df actual\n",
    "existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "if existing_numeric_cols:\n",
    "    correlation_matrix = df[existing_numeric_cols].corr()\n",
    "    print(correlation_matrix)\n",
    "\n",
    "    print(\"\\nMapa de Calor de Correlación:\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
    "    plt.title('Mapa de Calor de Correlación entre Variables Numéricas')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se encontraron columnas numéricas para calcular la correlación.\")\n",
    "\n",
    "\n",
    "# Diagramas de dispersión (Seleccionar pares relevantes)\n",
    "print(\"\\nDiagramas de Dispersión (Ejemplos):\")\n",
    "# Verificar si las columnas existen antes de intentar plotear\n",
    "scatter_pairs = [('Open', 'Close'), ('Volume', 'Volatility'), ('PE_Ratio', 'Dividend_Yield')]\n",
    "plt.figure(figsize=(18, 6))\n",
    "plot_index = 1\n",
    "for x_col, y_col in scatter_pairs:\n",
    "    if x_col in df.columns and y_col in df.columns:\n",
    "        plt.subplot(1, len(scatter_pairs), plot_index)\n",
    "        sns.scatterplot(data=df, x=x_col, y=y_col, alpha=0.5)\n",
    "        plt.title(f'{x_col} vs {y_col}')\n",
    "        plot_index += 1\n",
    "    else:\n",
    "        print(f\"Advertencia: No se pueden plotear '{x_col}' vs '{y_col}' (una o ambas columnas faltan).\")\n",
    "if plot_index > 1: # Solo mostrar si se ploteó algo\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se pudieron generar diagramas de dispersión de ejemplo.\")\n",
    "\n",
    "\n",
    "# --- Categórica vs. Categórica ---\n",
    "print(\"\\nAnálisis Categórica vs. Categórica:\")\n",
    "\n",
    "# Ejemplo: Sector vs Trend\n",
    "if 'Sector' in categorical_cols and 'Trend' in categorical_cols:\n",
    "    print(\"\\nTabla de Contingencia: Sector vs Trend\")\n",
    "    try:\n",
    "        crosstab_st = pd.crosstab(df['Sector'], df['Trend'])\n",
    "        print(crosstab_st)\n",
    "\n",
    "        # Gráfico de barras agrupadas\n",
    "        crosstab_st.plot(kind='bar', figsize=(12, 7), rot=45)\n",
    "        plt.title('Distribución de Trend por Sector')\n",
    "        plt.xlabel('Sector')\n",
    "        plt.ylabel('Conteo')\n",
    "        plt.legend(title='Trend')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Prueba Chi-cuadrado (opcional)\n",
    "        try:\n",
    "            chi2, p, dof, expected = stats.chi2_contingency(crosstab_st)\n",
    "            print(f\"\\nPrueba Chi-cuadrado (Sector vs Trend): Chi2={chi2:.2f}, p-value={p:.3f}\")\n",
    "            if p < 0.05:\n",
    "                print(\"Hay evidencia de una asociación significativa entre Sector y Trend (p < 0.05).\")\n",
    "            else:\n",
    "                print(\"No hay evidencia suficiente de una asociación significativa entre Sector y Trend (p >= 0.05).\")\n",
    "        except ValueError as ve:\n",
    "             print(f\"No se pudo realizar la prueba Chi-cuadrado (posiblemente por ceros en tabla): {ve}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error inesperado en la prueba Chi-cuadrado: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar la tabla de contingencia Sector vs Trend: {e}\")\n",
    "else:\n",
    "    print(\"No se encontraron las columnas 'Sector' y/o 'Trend' para el análisis Categórico-Categórico.\")\n",
    "\n",
    "\n",
    "# --- Numérica vs. Categórica ---\n",
    "print(\"\\nAnálisis Numérica vs. Categórica:\")\n",
    "\n",
    "# Ejemplo: Close vs Sector\n",
    "if 'Sector' in categorical_cols and 'Close' in existing_numeric_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Ordenar por mediana para mejor visualización\n",
    "    order = df.groupby('Sector')['Close'].median().sort_values().index\n",
    "    sns.boxplot(data=df, x='Sector', y='Close', order=order, palette='viridis')\n",
    "    plt.title('Distribución de Close por Sector (Ordenado por Mediana)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEstadísticas Descriptivas de 'Close' Agrupadas por 'Sector':\")\n",
    "    print(df.groupby('Sector')['Close'].describe())\n",
    "else:\n",
    "     print(\"No se encontraron las columnas 'Sector' y/o 'Close' para el análisis Numérico-Categórico.\")\n",
    "\n",
    "\n",
    "# Ejemplo: Sentiment_Score vs Trend\n",
    "if 'Trend' in categorical_cols and 'Sentiment_Score' in existing_numeric_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(data=df, x='Trend', y='Sentiment_Score', palette='muted', inner='quartile') # Mostrar cuartiles\n",
    "    plt.title('Distribución de Sentiment_Score por Trend')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nEstadísticas Descriptivas de 'Sentiment_Score' Agrupadas por 'Trend':\")\n",
    "    print(df.groupby('Trend')['Sentiment_Score'].describe())\n",
    "else:\n",
    "     print(\"No se encontraron las columnas 'Trend' y/o 'Sentiment_Score' para el análisis Numérico-Categórico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Outliers handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4. Identificación y Manejo de Outliers (Más a fondo)\n",
    "# ==================================================\n",
    "print(\"\\n----- 4. Identificación y Manejo de Outliers -----\")\n",
    "\n",
    "print(\"\\nIdentificación de Outliers usando el método IQR:\")\n",
    "# Revisaremos algunas columnas clave como ejemplo\n",
    "outlier_check_cols = ['Close', 'Volume', 'PE_Ratio', 'Volatility', 'Sentiment_Score', 'Market_Cap', 'Dividend_Yield']\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in outlier_check_cols:\n",
    "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]) and not df[col].isnull().all():\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Evitar errores si IQR es 0\n",
    "        if IQR > 0:\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_count = outliers.shape[0]\n",
    "            percentage = (outlier_count / df[col].notna().sum()) * 100 if df[col].notna().sum() > 0 else 0\n",
    "            outlier_summary[col] = {'count': outlier_count, 'percentage': percentage}\n",
    "\n",
    "            print(f\"\\nOutliers en '{col}': {outlier_count} ({percentage:.2f}%)\")\n",
    "            print(f\"  Límites (1.5*IQR): Inferior={lower_bound:.2f}, Superior={upper_bound:.2f}\")\n",
    "            if outlier_count > 0 and outlier_count < 15: # Mostrar algunos si no son demasiados\n",
    "                 print(\"  Algunos ejemplos de outliers:\")\n",
    "                 # Intentar mostrar Date si existe y es datetime\n",
    "                 cols_to_show = [col for col in [date_col,'Company', col] if col in outliers.columns]\n",
    "                 print(outliers[cols_to_show].head())\n",
    "            elif outlier_count >= 15:\n",
    "                 print(f\"  Se encontraron {outlier_count} outliers, no se muestran ejemplos por brevedad.\")\n",
    "                 print(f\"  Valor Mínimo Outlier (si existe < lower): {outliers[outliers[col] < lower_bound][col].min() if any(outliers[col] < lower_bound) else 'N/A'}\")\n",
    "                 print(f\"  Valor Máximo Outlier (si existe > upper): {outliers[outliers[col] > upper_bound][col].max() if any(outliers[col] > upper_bound) else 'N/A'}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n'{col}': IQR es 0, no se pueden calcular límites basados en IQR.\")\n",
    "            outlier_summary[col] = {'count': 0, 'percentage': 0.0}\n",
    "    elif col not in df.columns:\n",
    "         print(f\"\\nAdvertencia: La columna '{col}' no existe en el DataFrame.\")\n",
    "    elif not pd.api.types.is_numeric_dtype(df[col]):\n",
    "         print(f\"\\nAdvertencia: La columna '{col}' no es numérica.\")\n",
    "    else: # Es numérica pero todos son NaN\n",
    "         print(f\"\\nAdvertencia: La columna '{col}' solo contiene valores NaN.\")\n",
    "\n",
    "\n",
    "print(\"\\nResumen de Outliers (método 1.5*IQR):\")\n",
    "for col, data in outlier_summary.items():\n",
    "    print(f\"- {col}: {data['count']} outliers ({data['percentage']:.2f}%)\")\n",
    "\n",
    "\n",
    "print(\"\\nNota sobre el manejo de outliers:\")\n",
    "print(\"El tratamiento de outliers (eliminación, transformación, capping, uso de modelos robustos)\")\n",
    "print(\"depende críticamente del contexto del análisis, del dominio del problema y del objetivo final (ej: modelado).\")\n",
    "print(\"En este EDA inicial, solo los identificamos. Los boxplots del análisis univariado también ayudan a visualizarlos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Initial ideas and documentation ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5. Ingeniería de Características (Ideas Iniciales)\n",
    "# ==================================================\n",
    "print(\"\\n----- 5. Ingeniería de Características (Ideas Iniciales) -----\")\n",
    "\n",
    "# Usar df directamente, ya que no guardaremos cambios permanentemente en esta fase de EDA\n",
    "# df_feat = df.copy() # Descomentar si quieres mantener el df original intacto\n",
    "\n",
    "print(\"\\nCreando nuevas características a partir de la fecha:\")\n",
    "if date_col in df.columns and pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "    if 'Year' not in df.columns: df['Year'] = df[date_col].dt.year\n",
    "    if 'Month' not in df.columns: df['Month'] = df[date_col].dt.month\n",
    "    if 'Day' not in df.columns: df['Day'] = df[date_col].dt.day\n",
    "    if 'DayOfWeek' not in df.columns: df['DayOfWeek'] = df[date_col].dt.dayofweek # Lunes=0, Domingo=6\n",
    "    if 'WeekOfYear' not in df.columns: df['WeekOfYear'] = df[date_col].dt.isocalendar().week.astype(int)\n",
    "    print(\"Características de fecha creadas/verificadas: Year, Month, Day, DayOfWeek, WeekOfYear.\")\n",
    "    print(df[[date_col, 'Year', 'Month', 'Day', 'DayOfWeek', 'WeekOfYear']].head())\n",
    "else:\n",
    "    print(\"La columna 'Date' no es de tipo datetime o no existe. No se pueden crear características de fecha.\")\n",
    "\n",
    "print(\"\\nCreando características de rango y cambio de precio:\")\n",
    "required_price_cols = ['High', 'Low', 'Close', 'Open']\n",
    "if all(c in df.columns for c in required_price_cols):\n",
    "    if 'PriceRange' not in df.columns: df['PriceRange'] = df['High'] - df['Low']\n",
    "    if 'PriceChange' not in df.columns: df['PriceChange'] = df['Close'] - df['Open']\n",
    "    # Una característica potencialmente más útil: Cambio porcentual diario\n",
    "    if 'DailyReturn' not in df.columns and 'Open' in df.columns and df['Open'].ne(0).all(): # Evitar división por cero\n",
    "         df['DailyReturn'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "    print(\"Características de precio creadas/verificadas: PriceRange, PriceChange, DailyReturn.\")\n",
    "    cols_to_show = required_price_cols + ['PriceRange', 'PriceChange', 'DailyReturn']\n",
    "    print(df[[col for col in cols_to_show if col in df.columns]].head())\n",
    "else:\n",
    "    print(\"No se encontraron todas las columnas necesarias ('High', 'Low', 'Close', 'Open') para crear características de precio.\")\n",
    "\n",
    "print(\"\\nIdea adicional: Interacción Volumen * Volatilidad\")\n",
    "if 'Volume' in df.columns and 'Volatility' in df.columns:\n",
    "     if 'Volume_x_Volatility' not in df.columns:\n",
    "         df['Volume_x_Volatility'] = df['Volume'] * df['Volatility']\n",
    "         print(\"Característica 'Volume_x_Volatility' creada.\")\n",
    "         print(df[['Volume', 'Volatility', 'Volume_x_Volatility']].head())\n",
    "else:\n",
    "     print(\"No se encontraron 'Volume' y/o 'Volatility' para crear característica de interacción.\")\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 6. Documentación y Resumen\n",
    "# ==================================================\n",
    "print(\"\\n----- 6. Documentación y Resumen Final -----\")\n",
    "\n",
    "print(\"\\nResumen del Análisis Exploratorio de Datos (EDA):\")\n",
    "print(f\"- **Dataset:** Dimensiones finales {df.shape[0]} filas y {df.shape[1]} columnas (después de limpieza y feat. eng.).\") # Mostrar dimensiones finales\n",
    "print(\"- **Calidad de Datos:**\")\n",
    "print(f\"  - Valores Faltantes: {'No se encontraron.' if missing_values.sum() == 0 else f'Se encontraron {missing_values.sum()} valores faltantes (revisar manejo aplicado si lo hubo).'}\")\n",
    "print(f\"  - Duplicados: {'No se encontraron.' if duplicates == 0 else f'Se encontraron y eliminaron {duplicates} filas duplicadas.'}\")\n",
    "print(f\"  - Tipos de Datos: 'Date' convertida a datetime. Otros tipos parecen adecuados.\")\n",
    "\n",
    "print(\"- **Análisis Univariado:**\")\n",
    "print(\"  - Variables Numéricas: Se revisaron distribuciones (histogramas, boxplots), estadísticas y medidas de forma.\")\n",
    "print(\"    - Precios (Open, High, Low, Close): Distribuciones concentradas, poca asimetría aparente.\")\n",
    "print(\"    - Volume, Market_Cap, PE_Ratio, Dividend_Yield: A menudo muestran asimetría positiva (cola derecha) y outliers significativos (valores muy altos).\")\n",
    "print(\"    - Volatility: Parece tener una distribución más contenida, aunque puede tener algunos picos.\")\n",
    "print(\"    - Sentiment_Score: Distribución a menudo centrada cerca de cero, a veces bimodal o con colas, indicando opiniones mixtas/extremas.\")\n",
    "print(\"  - Variables Categóricas:\")\n",
    "print(f\"    - Company: Alta cardinalidad ({df['Company'].nunique()} empresas). El análisis por empresa requiere pasos adicionales.\")\n",
    "print(f\"    - Sector: Se observaron las frecuencias. Sectores como {df['Sector'].value_counts().index[0]}, {df['Sector'].value_counts().index[1]} son los más comunes.\")\n",
    "print(f\"    - Trend: Distribución entre {', '.join(df['Trend'].unique())} analizada. Proporciones: {df['Trend'].value_counts(normalize=True).mul(100).round(1).to_dict()}\")\n",
    "\n",
    "print(\"- **Análisis Bivariado:**\")\n",
    "print(\"  - Numérico-Numérico: Fuerte correlación positiva entre variables de precio. Correlaciones más débiles/variables entre Volumen, Volatilidad, PE_Ratio, etc. (ver heatmap).\")\n",
    "print(\"  - Categórico-Categórico: La relación Sector-Trend (si se calculó) mostró si ciertas tendencias predominan en algunos sectores (ver gráfico/Chi2).\")\n",
    "print(\"  - Numérico-Categórico: Se observaron diferencias en los precios medios/medianos ('Close') y sentimiento ('Sentiment_Score') entre Sectores y Tendencias (ver boxplots/violinplots).\")\n",
    "\n",
    "print(\"- **Análisis Multivariado:**\")\n",
    "print(\"  - Pairplot (sobre muestra) permitió visualizar múltiples relaciones simultáneamente, usando el Sector para diferenciar puntos.\")\n",
    "\n",
    "print(\"- **Outliers:**\")\n",
    "print(f\"  - Se identificaron outliers (método 1.5*IQR) en columnas como: {[col for col, data in outlier_summary.items() if data['count'] > 0]}.\")\n",
    "print(f\"  - Columnas como 'Volume', 'Market_Cap', 'PE_Ratio', 'Dividend_Yield' mostraron un porcentaje significativo de outliers, sugiriendo distribuciones de cola larga o valores extremos reales.\")\n",
    "print(\"  - El manejo adecuado (transformación logarítmica, winsorización, etc.) debe considerarse antes del modelado si estos valores afectan los supuestos del modelo.\")\n",
    "\n",
    "print(\"- **Ingeniería de Características:**\")\n",
    "print(\"  - Se generaron características de fecha (Year, Month, etc.) y de precio/volumen (PriceRange, DailyReturn, Volume_x_Volatility) que enriquecen el dataset para análisis temporal o modelado.\")\n",
    "\n",
    "print(\"\\n**Próximos Pasos Sugeridos (Reiteración/Refinamiento):**\")\n",
    "print(\"1. **Análisis por Compañía/Sector:** Agrupar datos y repetir análisis clave para compañías o sectores específicos de interés.\")\n",
    "print(\"2. **Análisis Temporal:** Graficar variables clave (ej: Close, Volume, DailyReturn) contra 'Date'. Buscar tendencias, estacionalidad, autocorrelación (ACF/PACF). Calcular medias móviles.\")\n",
    "print(\"3. **Manejo Profundo de Outliers:** Investigar la naturaleza de los outliers (¿errores o valores extremos reales?). Aplicar técnicas de tratamiento si es necesario para el modelado (ej: log-transform para variables con asimetría positiva, winsorizing).\")\n",
    "print(\"4. **Ingeniería de Características Adicional:** Crear variables rezagadas (lags) de precios o retornos, indicadores técnicos (RSI, MACD), variables dummy para eventos específicos (si se conocen).\")\n",
    "print(\"5. **Selección/Preparación para Modelo:** Basado en los insights (correlaciones, importancia visual), seleccionar características relevantes. Realizar escalado (StandardScaler, MinMaxScaler) y codificación (OneHotEncoder) según el modelo a utilizar.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
